{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Perform OOD evaluation of single-nucleotide variant effects using CAGI5 challenge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shush/miniconda3/envs/pytorch4/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "from einops import rearrange\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "\n",
    "from genomic_augmentations import models, supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_np(whole_dataset, batch_size):\n",
    "    \"\"\"\n",
    "    batch a np array for passing to a model without running out of memory\n",
    "    :param whole_dataset: np array dataset\n",
    "    :param batch_size: batch size\n",
    "    :return: generator of np batches\n",
    "    \"\"\"\n",
    "    for i in range(0, whole_dataset.shape[0], batch_size):\n",
    "        yield whole_dataset[i:i + batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_data = 'CAGI_onehot.h5'\n",
    "f =  h5py.File(vcf_data, \"r\")\n",
    "alt_3k = f['alt'][()]\n",
    "ref_3k = f['ref'][()]\n",
    "f.close()\n",
    "\n",
    "window_size = 600\n",
    "L = alt_3k.shape[1]\n",
    "alt_3k_crop = alt_3k[:,(L//2-window_size//2):(L//2+window_size//2), :]\n",
    "ref_3k_crop = ref_3k[:,(L//2-window_size//2):(L//2+window_size//2), :]\n",
    "ref_3k_crop = rearrange(ref_3k_crop, 'a b c -> a c b')\n",
    "alt_3k_crop = rearrange(alt_3k_crop, 'a b c -> a c b')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model (and its `Config` file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"/home/nick/Results/Basset/Models\"\n",
    "model_supervised = \"Basset\"\n",
    "dataset = \"Basset\"\n",
    "trials = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/85 [00:00<?, ?it/s]/home/shush/miniconda3/envs/pytorch4/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "/home/shush/miniconda3/envs/pytorch4/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py:22: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/85 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.76 GiB total capacity; 9.61 GiB already allocated; 13.44 MiB free; 9.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         ref \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(ref)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m         alt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(alt)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m         ref_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     42\u001b[0m         alt_pred \u001b[38;5;241m=\u001b[39m model_inference(alt)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#         ref_pred, alt_pred = save_output.outputs\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#         vcf_diff = alt_pred / ref_pred\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch4/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/project_nick/genomic_augmentations/models.py:387\u001b[0m, in \u001b[0;36mSupervisedModelWithPadding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    386\u001b[0m     x_padded \u001b[38;5;241m=\u001b[39m aug_pad_end(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minsert_max) \u001b[38;5;66;03m# if \"insert\" in self.augmentation_string else x\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_padded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_hat\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch4/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/project_nick/genomic_augmentations/supervised.py:190\u001b[0m, in \u001b[0;36mBasset.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# Layer 1\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     cnn \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1_filters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1_filters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     cnn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm1(cnn)\n\u001b[1;32m    192\u001b[0m     cnn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation1(cnn)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.76 GiB total capacity; 9.61 GiB already allocated; 13.44 MiB free; 9.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# list of all models\n",
    "all_models = glob.glob(f'{models_dir}/{model_supervised}_{dataset}*_Model.ckpt')\n",
    "\n",
    "h5f_output = h5py.File('predict_one_shot.h5', 'w')\n",
    "# Determine paths to trained model checkpoint and its config file\n",
    "for checkpoint_path in tqdm(all_models):\n",
    "    all_ref_preds = []\n",
    "    all_alt_preds = []\n",
    "    config_path = checkpoint_path.replace('Finetune_', '').replace('_Model.ckpt', '_Config.p')\n",
    "    model_label = checkpoint_path.replace('_Model.ckpt', '').split('/')[-1]\n",
    "    # Load model config (from which loss function and augmentation type and hyperparameters are drawn)\n",
    "    config_supervised_dict = pickle.load( open(config_path, \"rb\") )\n",
    "    config_supervised = Namespace(**config_supervised_dict)\n",
    "    # Set loss function\n",
    "    losstype_lower = config_supervised.loss.lower()\n",
    "    if losstype_lower == \"bce\": \n",
    "        loss = torch.nn.BCELoss()\n",
    "    elif losstype_lower == \"mse\":\n",
    "        loss = torch.nn.MSELoss()\n",
    "    else:\n",
    "        raise ValueError(\"unrecognized loss function type: %s\" % loss)\n",
    "    # Load model for inference using checkpoint\n",
    "    numclasses_Basset = 164\n",
    "    model_untrained = supervised.Basset(numclasses_Basset) # denotes model architecture on which to load checkpoint\n",
    "\n",
    "    if \"insert\" in config_supervised.augs:\n",
    "        print('insert')\n",
    "        model_inference = models.SupervisedModelWithPadding.load_from_checkpoint(checkpoint_path=checkpoint_path, \n",
    "                                                                                 model_untrained=model_untrained, loss_criterion=loss,\n",
    "                                                                                 insert_max=config_supervised.insert_max).to('cuda')\n",
    "    else: # all other data augmentations do not affect expected input sequence length\n",
    "        model_inference = models.SupervisedModel.load_from_checkpoint(checkpoint_path=checkpoint_path, \n",
    "                                                                      model_untrained=model_untrained, loss_criterion=loss).to('cuda')\n",
    "\n",
    "    model_inference.eval();\n",
    "\n",
    "    batch_size = 16\n",
    "    for ref, alt in zip(batch_np(ref_3k_crop, batch_size), batch_np(alt_3k_crop, batch_size)):\n",
    "\n",
    "        # reference allele predictions\n",
    "        ref = torch.from_numpy(ref).float().to('cuda')\n",
    "        ref_pred = model_inference(ref).to('cpu')\n",
    "        all_ref_preds.append(ref_pred)\n",
    "        \n",
    "        # alternative allele predictions\n",
    "        alt = torch.from_numpy(alt).float().to('cuda')\n",
    "        alt_pred = model_inference(alt).to('cpu')\n",
    "        all_alt_preds.append(alt_pred)\n",
    "    all_ref_preds = np.concatenate(all_ref_preds)\n",
    "    all_alt_preds = np.concatenate(all_alt_preds)\n",
    "\n",
    "    # save outputs\n",
    "    h5f_output.create_dataset('ref_'+model_label, data=all_ref_preds)\n",
    "    h5f_output.create_dataset('alt_'+model_label, data=all_alt_preds)\n",
    "h5f_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental values\n",
    "cagi_df = pd.read_csv('final_cagi_metadata.csv',\n",
    "                      index_col=0).reset_index()\n",
    "experimental_log_fold_change = cagi_df['6'].values\n",
    "\n",
    "# predicted values\n",
    "h5f_output = h5py.File('predict_one_shot.h5', 'r')\n",
    "model_keys = list(h5f_output.keys())\n",
    "\n",
    "for i in range(len(model_keys)):\n",
    "    example_log_fold_change = h5f_output[model_keys[i]][:]\n",
    "    print(i, pearsonr(experimental_log_fold_change, example_log_fold_change[:,0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
